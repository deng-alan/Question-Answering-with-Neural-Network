{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cPickle as pickle\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read all the tables\n",
    "table_path = '../data/WikiTableQuestions/csv/'\n",
    "\n",
    "def clean_text(s):\n",
    "    return s.lower().replace('\\\\n', ' ').replace('\\n', '')\n",
    "    \n",
    "def read_table(filename, path=table_path, delimiter='\\t'):\n",
    "    ''' given the path and file name of certain table, \n",
    "        return a dictionary with {head-title: [elements]} \n",
    "    '''\n",
    "    result = {}\n",
    "    with open(path+filename, 'r') as f:\n",
    "        heads, head_index = f.readline().split(delimiter), []\n",
    "        for head in heads:\n",
    "            head = clean_text(head)\n",
    "            result[head] = []\n",
    "            head_index.append(head)\n",
    "        \n",
    "        for line in f:\n",
    "            for head, ele in zip(head_index, line.split(delimiter)):\n",
    "                ele = clean_text(ele)\n",
    "                result[head].append(ele)\n",
    "    return result\n",
    "\n",
    "def read_all_tables(table_path=table_path):\n",
    "    all_tables = {}\n",
    "    for path in glob.glob(table_path+'*'):\n",
    "        csv_path = path[-7:]\n",
    "        for f in glob.glob(table_path+csv_path+'/*.tsv'):\n",
    "            filename = f.split('/')[-1]\n",
    "            context = 'csv/{0}/{1}csv'.format(csv_path, filename[:-3])\n",
    "            table = read_table(csv_path + '/' + filename)\n",
    "            all_tables[context] = table\n",
    "    return all_tables\n",
    "\n",
    "all_tables = read_all_tables()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Preprocessing, not need to re-run\n",
    "Proprocess all the questions to format:  \n",
    "{ 'id': {\n",
    "      'question': question (string),  \n",
    "      'table_id': table_id (string),  \n",
    "      'answer': answer (string),  \n",
    "      'answer_position': {column name: [row indices]} # columns, row indices: might have more than one matches,  \n",
    "      'syntax': {  \n",
    "          index: {word, tag, pos, role, parent, [child]}\n",
    "      }  \n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_answer(answer, table):\n",
    "    ''' return answer_position: {column name: [row indices]}\n",
    "    '''\n",
    "    result = {}\n",
    "    for head in table:\n",
    "        indices = [i for i, text in enumerate(table[head]) if text==answer.lower()]\n",
    "        if len(indices)!=0:\n",
    "            result[head] = indices\n",
    "    return result\n",
    "\n",
    "def process_syntax(syntax):\n",
    "    result = {}\n",
    "    for index, info in enumerate(syntax[1]):\n",
    "        result[index] = {'word': info[0], 'tag': info[1], 'pos': info[2], \n",
    "                         'role': info[3], 'parent': None, 'children': []}\n",
    "    for index, children in enumerate(syntax[2]):\n",
    "        for child in children:\n",
    "            result[index]['children'].append(child)\n",
    "            result[child]['parent'] = index\n",
    "    return result\n",
    "    \n",
    "def question_preprocess(loadfile, savefile):\n",
    "    with open(loadfile, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    result = {}\n",
    "    for info in data:\n",
    "        id, question, table_id, answer, syntax = info\n",
    "        answer_position = find_answer(answer, all_tables[table_id])\n",
    "        syntax = process_syntax(syntax)\n",
    "        result[id] = {\n",
    "            'question': question,\n",
    "            'table_id': table_id,\n",
    "            'answer': answer,\n",
    "            'answer_position': answer_position,\n",
    "            'syntax': syntax\n",
    "        }\n",
    "    with open(savefile, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syntaxnet = '../data/questions_syntaxnet/'\n",
    "questions = '../data/PreprocessedQuestions/'\n",
    "files = ['training.pkl', 'seen-tables.pkl', 'unseen-tables.pkl']\n",
    "for filename in files:\n",
    "    question_preprocess(syntaxnet+filename, questions+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Astarto',\n",
       " 'answer_position': {'title': [16]},\n",
       " 'question': 'which opera has the most acts, la fortezza al cimento or astarto?',\n",
       " 'syntax': {0: {'children': [],\n",
       "   'parent': 1,\n",
       "   'pos': 'WDT',\n",
       "   'role': 'det',\n",
       "   'tag': 'DET',\n",
       "   'word': 'which'},\n",
       "  1: {'children': [0],\n",
       "   'parent': 2,\n",
       "   'pos': 'NN',\n",
       "   'role': 'nsubj',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'opera'},\n",
       "  2: {'children': [1, 5, 13],\n",
       "   'parent': None,\n",
       "   'pos': 'VBZ',\n",
       "   'role': 'ROOT',\n",
       "   'tag': 'VERB',\n",
       "   'word': 'has'},\n",
       "  3: {'children': [],\n",
       "   'parent': 5,\n",
       "   'pos': 'DT',\n",
       "   'role': 'det',\n",
       "   'tag': 'DET',\n",
       "   'word': 'the'},\n",
       "  4: {'children': [],\n",
       "   'parent': 5,\n",
       "   'pos': 'JJS',\n",
       "   'role': 'amod',\n",
       "   'tag': 'ADJ',\n",
       "   'word': 'most'},\n",
       "  5: {'children': [3, 4, 6, 10],\n",
       "   'parent': 2,\n",
       "   'pos': 'NNS',\n",
       "   'role': 'dobj',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'acts'},\n",
       "  6: {'children': [],\n",
       "   'parent': 5,\n",
       "   'pos': ',',\n",
       "   'role': 'punct',\n",
       "   'tag': '.',\n",
       "   'word': ','},\n",
       "  7: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'FW',\n",
       "   'role': 'advmod',\n",
       "   'tag': 'X',\n",
       "   'word': 'la'},\n",
       "  8: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'FW',\n",
       "   'role': 'nn',\n",
       "   'tag': 'X',\n",
       "   'word': 'fortezza'},\n",
       "  9: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'NNP',\n",
       "   'role': 'nn',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'al'},\n",
       "  10: {'children': [7, 8, 9, 11, 12],\n",
       "   'parent': 5,\n",
       "   'pos': 'NNP',\n",
       "   'role': 'appos',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'cimento'},\n",
       "  11: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'CC',\n",
       "   'role': 'cc',\n",
       "   'tag': 'CONJ',\n",
       "   'word': 'or'},\n",
       "  12: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'NN',\n",
       "   'role': 'conj',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'astarto'},\n",
       "  13: {'children': [],\n",
       "   'parent': 2,\n",
       "   'pos': '.',\n",
       "   'role': 'punct',\n",
       "   'tag': '.',\n",
       "   'word': '?'}},\n",
       " 'table_id': 'csv/204-csv/104.csv'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to load data + data schema\n",
    "with open('../data/PreprocessedQuestions/training.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "data[data.keys()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Find the correct column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load word embedding\n",
    "word_vectors = {}\n",
    "with open('../glove.840B.300d.txt', 'r') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        i += 1\n",
    "        if i % 100000==0: \n",
    "            clear_output()\n",
    "            print 'Processing {0}th word'.format(i)\n",
    "        info = line.split()\n",
    "        word_vectors[info[0]] = np.array([float(x) for x in info[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the 'W*' word, return the index\n",
    "def find_w(syntax):\n",
    "    for index in syntax:\n",
    "        if syntax[index]['pos'][0] == 'W':\n",
    "            return index\n",
    "    return -1 # could not find w word\n",
    "\n",
    "# find the 'nearest' noun of 'W*' word\n",
    "def find_answer_noun(syntax):\n",
    "    w_index = find_w(syntax)\n",
    "    if syntax[w_index]['pos'][0] == 'N':\n",
    "        return w_index\n",
    "    \n",
    "    parent = syntax[w_index]['parent']\n",
    "    children = syntax[w_index]['children']\n",
    "    que = children\n",
    "    que.append(parent)\n",
    "    visited = [w_index, None]\n",
    "    \n",
    "    # Use BFS to find the nearest noun\n",
    "    while len(que) != 0:\n",
    "        index = que.pop(0)\n",
    "        if index == None: continue\n",
    "        if index not in visited:\n",
    "            visited.append(index)\n",
    "            if syntax[index]['pos'][0] == 'N':\n",
    "                return index\n",
    "        new_child = syntax[index]['children']\n",
    "        for child in new_child:\n",
    "            if child not in visited:\n",
    "                que.append(child)\n",
    "        new_parent = syntax[index]['parent']\n",
    "        if new_parent not in visited:\n",
    "            que.append(new_parent)\n",
    "    return -1 # cound not find closest noun\n",
    "\n",
    "# for different 'W*' word, given the object key word\n",
    "def find_object_word(syntax):\n",
    "    w_index = find_w(syntax)\n",
    "    if w_index == -1: return ''\n",
    "    \n",
    "    w_word = syntax[w_index]['word']\n",
    "    if w_word == 'when': return 'year'\n",
    "    if w_word == 'who': return 'name'\n",
    "    if w_word == 'where': return 'location'\n",
    "    \n",
    "    # If w_word == 'how' or 'what'\n",
    "    index = find_answer_noun(syntax)\n",
    "    if index == -1: return ''\n",
    "    \n",
    "    return syntax[index]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(w1, w2, word_vectors=word_vectors):\n",
    "    w1, w2 = w1.lower().split(), w2.lower().split()\n",
    "    dist = 0.0\n",
    "    for ww1 in w1:\n",
    "        if ww1 not in word_vectors: continue\n",
    "        for ww2 in w2:\n",
    "            if ww1 == ww2: return 1.0\n",
    "            if ww2 not in word_vectors: continue\n",
    "            v1, v2 = word_vectors[ww1], word_vectors[ww2]\n",
    "            product = np.dot(v1,v2)\n",
    "            length = np.sqrt(np.dot(v1,v1)*np.dot(v2,v2))\n",
    "            if length == 0: return 1.0\n",
    "            dist = max(dist, product/length)                \n",
    "    return dist\n",
    "\n",
    "def distance_similarity(w1, w2, word_vectors=word_vectors):\n",
    "    w1, w2 = w1.lower(), w2.lower()\n",
    "    if w1==w2: return float('inf')\n",
    "    if w1 not in word_vectors or w2 not in word_vectors:\n",
    "        return 0.0\n",
    "    v1, v2 = word_vectors[w1], word_vectors[w2]\n",
    "    return 1.0/max(0.01, np.sqrt(np.sum((v1-v2)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rank_head_answer_noun_similarity(question, similarity_function = cosine_similarity):\n",
    "    syntax = question['syntax']\n",
    "    answer_noun = find_object_word(syntax)\n",
    "    \n",
    "    if question['answer_position'] == {}:\n",
    "        return (-1, None, answer_noun)\n",
    "    elif answer_noun == '':\n",
    "        return (-2, None, '')\n",
    "    \n",
    "    head = question['answer_position'].keys()[0]\n",
    "    similarity = similarity_function(answer_noun, head)\n",
    "    \n",
    "    result = (-1, None, answer_noun)\n",
    "    for h in all_tables[question['table_id']].keys():\n",
    "        sim = similarity_function(answer_noun, h)\n",
    "        if sim >= similarity: \n",
    "            result = (result[0]+1, h, answer_noun)\n",
    "    if result[1].lower()==head.lower():\n",
    "        result = (0, head, answer_noun)\n",
    "    #w_index = find_w(syntax)\n",
    "    #w_word = syntax[w_index]['word']\n",
    "    #print w_word, result\n",
    "    return result\n",
    "\n",
    "def rank_distribution(questions):\n",
    "    return [(id, rank_head_answer_noun_similarity(question)) for id, question in questions.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Brief Analysis \n",
    "with open('../data/PreprocessedQuestions/training.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "dist = rank_distribution(data)\n",
    "x = [a[1][0] for a in dist]\n",
    "_ = plt.hist(x, bins=max(x)-min(x))\n",
    "\n",
    "# -1: answer cannot be directly found\n",
    "# -2: can't find w word or can't find answer noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions_path = '../data/PreprocessedQuestions/'\n",
    "files = ['training.pkl', 'seen-tables.pkl', 'unseen-tables.pkl']\n",
    "column_rank = {}\n",
    "for filename in files:\n",
    "    with open(questions_path+filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    dist = rank_distribution(data)\n",
    "    \n",
    "    column_rank[filename[:-4]] = dist\n",
    "    \n",
    "    x = [a[1][0] for a in dist]\n",
    "    \n",
    "    count_nouns = 0\n",
    "    count_ans = 0\n",
    "    \n",
    "    for number in x:\n",
    "        if number < 0:\n",
    "            count_nouns += 1\n",
    "        else:\n",
    "            count_ans += 1\n",
    "    \n",
    "    print 'We could correctly find {0:.1f}% of clolumns in {1}.'.format(100*float(sum(np.array(x)==0))/count_ans, filename[:-4])\n",
    "    print '\\t', sum(np.array(x)<0), sum(np.array(x)==0), sum(np.array(x)>0)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The questions that get wrong columns\n",
    "def str_question_with_wrong_columns(question, result):\n",
    "    if result[0]==0:\n",
    "        return None\n",
    "    if result[0]<0:\n",
    "        return None\n",
    "    return 'Answer Column: {0}, Predict Column: {1}, Rank: {2}'.format(\n",
    "        question['answer_position'].keys()[0], result[1], result[0]+1)\n",
    "\n",
    "def debug(filename):\n",
    "    with open(questions_path+filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print filename[:-4], ':'\n",
    "\n",
    "    w_list = ['who', 'when', 'what', 'where', 'how', 'why']\n",
    "    w_dict = {'who':0, 'when':0, 'what':0, 'where':0, 'how':0, 'why':0, 'other':0}\n",
    "    w_dict_wrong = {'who':0, 'when':0, 'what':0, 'where':0, 'how':0, 'why':0, 'other':0}\n",
    "    \n",
    "    for id, result in column_rank[filename[:-4]]:\n",
    "        question = data[id]\n",
    "        debug_str = str_question_with_wrong_columns(question, result)\n",
    "        w_word = question['syntax'][0]['word']\n",
    "            \n",
    "        if result[0] == 0:\n",
    "            if w_word in w_list:\n",
    "                w_dict[w_word]+=1\n",
    "            else:\n",
    "                w_dict['other']+=1\n",
    "        if debug_str is not None:\n",
    "            if w_word in w_list:\n",
    "                w_dict_wrong[w_word]+=1\n",
    "            else:\n",
    "                w_dict_wrong['other']+=1\n",
    "            #print ' \\tAnswer Noun: ' + result[2]\n",
    "            #print ' \\t' + debug_str\n",
    "            #print ' \\t' + question['question']\n",
    "            #print\n",
    "    for key in w_dict:\n",
    "        print key, w_dict[key], w_dict_wrong[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'training.pkl'\n",
    "debug(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'seen-tables.pkl'\n",
    "debug(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'unseen-tables.pkl'\n",
    "debug(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Find the answer (clue column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cos_sim_forComplex(w1_list, w2):\n",
    "    w2_complex = w2.split()\n",
    "    dist = []\n",
    "    temp_dist = 0.0\n",
    "    for word2 in w2_complex:        \n",
    "        for word1 in w1_list:\n",
    "            if word2 not in word_vectors: break\n",
    "            if word1 not in word_vectors: continue\n",
    "            if word2 == word1: \n",
    "                temp_dist = 1.0\n",
    "                break\n",
    "            v1, v2 = word_vectors[word1], word_vectors[word2]\n",
    "            product = np.dot(v1,v2)\n",
    "            length = np.sqrt(np.dot(v1,v1)*np.dot(v2,v2))\n",
    "            if length == 0: \n",
    "                temp_dist = 1.0\n",
    "                break\n",
    "            temp_dist = max(temp_dist, product/length)\n",
    "        dist.append(temp_dist)\n",
    "        temp_dist = 0.0\n",
    "    return sum(dist)\n",
    "    \n",
    "\n",
    "def findClueList(syntax, key_word):\n",
    "    noun_list = []\n",
    "    # Find all of the nouns in the sentence\n",
    "    for key in syntax:\n",
    "        if syntax[key]['pos'][0] == 'N' and syntax[key]['word'] != key_word:\n",
    "            noun_list.append(syntax[key]['word'])\n",
    "    return noun_list\n",
    "    \n",
    "def findNearestCluePair(clue_list, col_list, similarity_function = cosine_similarity):\n",
    "    cos_dist, column_word = 0.0, ''\n",
    "    \n",
    "    for column in col_list:\n",
    "        dist = cos_sim_forComplex(clue_list, column)\n",
    "        if dist > cos_dist:\n",
    "            cos_dist, column_word = dist, column    \n",
    "    return column_word\n",
    "\n",
    "def findAns(question, column, key_word):    \n",
    "    ans_table = all_tables[question['table_id']]\n",
    "    ans_column = ans_table[column]\n",
    "    syntax = question['syntax']\n",
    "    \n",
    "    question_set = set(question['question'].split())\n",
    "    \n",
    "    clue_list = findClueList(syntax, key_word)\n",
    "    column_list = ans_table.keys()\n",
    "    column_list.remove(column)\n",
    "    \n",
    "    clue_column = findNearestCluePair(clue_list, column_list)\n",
    "    \n",
    "    if clue_column == '': return '', question['answer']\n",
    "    clue_column_list = ans_table[clue_column]\n",
    "    ans_index = 0\n",
    "    for index, clue_cell in enumerate(clue_column_list):\n",
    "        clue_cell_set = set(clue_cell.split())\n",
    "        if clue_cell_set.issubset(question_set) and index < len(ans_column):\n",
    "            ans_index = index\n",
    "            break\n",
    "    return ans_column[ans_index], question['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printResult(filename, n_a, wrong_column, wrong_answer, right_answer):\n",
    "    print filename\n",
    "    print '  {}% of questions with correct column have correct answer.'.format(float(right_answer)/(wrong_answer+right_answer)*100)\n",
    "    print '  {}% of questions have correct column.'.format(float(wrong_answer+right_answer)/(wrong_answer+right_answer+wrong_column)*100) \n",
    "    print '  {}% of questions have correct answer.'.format(float(right_answer)/(wrong_answer+right_answer+wrong_column)*100)\n",
    "    print '  Right answer: {}'.format(right_answer)\n",
    "    print '  Wrong answer: {}'.format(wrong_answer)\n",
    "    print '  Not the correct column: {}'.format(wrong_column)\n",
    "    print '  Not applicable: {}'.format(n_a)\n",
    "    \n",
    "def answerQuestions(questions):\n",
    "    n_a, wrong_column, wrong_answer, right_answer = 0, 0, 0, 0\n",
    "    for id, question in questions.items():\n",
    "        rank, column, key_word = rank_head_answer_noun_similarity(question)\n",
    "        if rank < 0:\n",
    "            n_a += 1\n",
    "        elif rank > 0:\n",
    "            wrong_column += 1\n",
    "        else: # rank == 0\n",
    "            found, real = findAns(question, column, key_word)\n",
    "            if found.lower() == real.lower():\n",
    "                right_answer += 1\n",
    "            else:\n",
    "                wrong_answer += 1\n",
    "    return (n_a, wrong_column, wrong_answer, right_answer)\n",
    "\n",
    "def processFiles(questions_path, files):    \n",
    "    for filename in files:        \n",
    "        with open(questions_path+filename, 'rb') as f:\n",
    "            questions = pickle.load(f)\n",
    "        n_a, wrong_column, wrong_answer, right_answer = answerQuestions(questions)\n",
    "        printResult(filename, n_a, wrong_column, wrong_answer, right_answer)\n",
    "    return 'Analysis Complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions_path = '../data/PreprocessedQuestions/'\n",
    "files = ['training.pkl', 'seen-tables.pkl', 'unseen-tables.pkl']\n",
    "processFiles(questions_path, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
