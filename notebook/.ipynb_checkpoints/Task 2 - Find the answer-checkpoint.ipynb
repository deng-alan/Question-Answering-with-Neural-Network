{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cPickle as pickle\n",
    "from IPython.display import clear_output\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data path\n",
    "data_path = '../data/WikiTableQuestions/'\n",
    "table_path = data_path + 'csv/'\n",
    "question_path = data_path + 'data/'\n",
    "\n",
    "# POS tagging\n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TOOLS:\n",
    "# SyntexNet\n",
    "# How exactly the syntexnet works?\n",
    "def syntexnet(sentence, tmp_file='/home/yuweilin/tmp/syntaxnet_temp.txt'):\n",
    "    ''' - sentence: string\n",
    "        @ return [root, words_info, hierarchy]\n",
    "            - word_info: (word, 'PRON', 'WP', 'nsubj')\n",
    "            - hierarchy: the next level\n",
    "    '''\n",
    "    \n",
    "    bashCommand = 'echo \\'' + sentence + '\\' | ./syntaxnet/demo.test.sh >' + tmp_file\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir('/home/yuweilin/models/syntaxnet/')\n",
    "    os.system(bashCommand)\n",
    "    os.chdir(cwd)\n",
    "    with open(tmp_file, 'r') as f:\n",
    "        lines = f.readlines()[:-1]\n",
    "        root, word_info, hierarchy = -1, [(None, None, None, None)]*len(lines), [[] for _ in range(len(lines))]\n",
    "        for line in lines:\n",
    "            index, word, _, info1, info2, _, parent, info3, _, _ = line.split()\n",
    "            index, parent = int(index)-1, int(parent)-1\n",
    "            word_info[index] = (word, info1, info2, info3)\n",
    "            if parent == -1:\n",
    "                root = index\n",
    "            else:\n",
    "                hierarchy[parent].append(index)\n",
    "    return [root, word_info, hierarchy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    return s.replace('\\\\n', ' ').replace('\\n', '')\n",
    "    \n",
    "def read_table(filename, path=table_path, delimiter='\\t'):\n",
    "    ''' given the path and file name of certain table, return a dictionary with {head-title: [elements]} \n",
    "    '''\n",
    "    result = {}\n",
    "    with open(path+filename, 'r') as f:\n",
    "        heads, head_index = f.readline().split(delimiter), []\n",
    "        for head in heads:\n",
    "            head = clean_text(head)\n",
    "            result[head] = []\n",
    "            head_index.append(head)\n",
    "        \n",
    "        for line in f:\n",
    "            for head, ele in zip(head_index, line.split(delimiter)):\n",
    "                ele = clean_text(ele)\n",
    "                result[head].append(ele)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def read_questions(filename, path=question_path, delimiter='\\t'):\n",
    "    ''' given the path and file name of certain questions \n",
    "    '''\n",
    "    with open(path+filename, 'r') as f:\n",
    "        f.readline()\n",
    "        result = []\n",
    "        for line in f:\n",
    "            result.append([clean_text(s) for s in line.split(delimiter)])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process all the questions through SyntexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = ['training.tsv', 'pristine-seen-tables.tsv', 'pristine-unseen-tables.tsv']\n",
    "for filename in filenames:\n",
    "    i, result = 0, []\n",
    "    sub = read_questions(filename)\n",
    "    size = len(sub)\n",
    "    for id, question, table_id, answer in sub:\n",
    "        i += 1\n",
    "        if i%10==0: \n",
    "            clear_output()\n",
    "            print 'Processing {0}/{2} question, {1}%'.format(i, 100.0*i/size, size)\n",
    "        precessed_question = syntexnet(question)\n",
    "        result.append([id, question, table_id, answer, precessed_question])\n",
    "    with open('processed_'+filename, 'wb') as f:\n",
    "        pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load glo-ve word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2100000th word\n"
     ]
    }
   ],
   "source": [
    "word_vectors = {}\n",
    "with open('/home/yuweilin/word-embedding/glove.840B.300d.txt', 'r') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        i += 1\n",
    "        if i % 100000==0: \n",
    "            clear_output()\n",
    "            print 'Processing {0}th word'.format(i)\n",
    "        info = line.split()\n",
    "        word_vectors[info[0]] = np.array([float(x) for x in info[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count of direct answers\n",
    "Find out how many answers are directly provided in the tabel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read all the tables and construct a set of existing words for each table\n",
    "table_words = {}\n",
    "for path in glob.glob(table_path+'*'):\n",
    "    csv_path = path[-7:]\n",
    "    for f in glob.glob(table_path+csv_path+'/*.tsv'):\n",
    "        filename = f.split('/')[-1]\n",
    "        context = 'csv/{0}/{1}csv'.format(csv_path, filename[:-3])\n",
    "        table = read_table(csv_path + '/' + filename)\n",
    "        words = set()\n",
    "        for key, value in table.items():\n",
    "            words |= set([key]+value)\n",
    "        table_words[context] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare the answer of each question with the word set of corresponding table\n",
    "def statsCountDirect(filename):\n",
    "    has, hasNot = [], []\n",
    "    for id, utterance, context, targetValue in read_questions(filename):\n",
    "        if context not in table_words:\n",
    "            print('Could not find table {0}'.format(context))\n",
    "            continue\n",
    "        if targetValue in table_words[context]:\n",
    "            has.append(id)\n",
    "        else:\n",
    "            hasNot.append(id)\n",
    "    print filename\n",
    "    print '  Answer/target of {}% of questions can directly be found in the corresponding table.'.format(\n",
    "        float(len(has))/(len(has)+len(hasNot))*100)\n",
    "    print '  # of questions has directed answer = {0}'.format(len(has))\n",
    "    print '  # of questions has no directed answer = {0}'.format(len(hasNot))\n",
    "    return (has, hasNot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.tsv\n",
      "  Answer/target of 61.9912379876% of questions can directly be found in the corresponding table.\n",
      "  # of questions has directed answer = 8773\n",
      "  # of questions has no directed answer = 5379\n",
      "pristine-seen-tables.tsv\n",
      "  Answer/target of 61.1817924795% of questions can directly be found in the corresponding table.\n",
      "  # of questions has directed answer = 2164\n",
      "  # of questions has no directed answer = 1373\n",
      "pristine-unseen-tables.tsv\n",
      "  Answer/target of 61.97053407% of questions can directly be found in the corresponding table.\n",
      "  # of questions has directed answer = 2692\n",
      "  # of questions has no directed answer = 1652\n"
     ]
    }
   ],
   "source": [
    "_ = statsCountDirect('training.tsv')\n",
    "_ = statsCountDirect('pristine-seen-tables.tsv')\n",
    "_ = statsCountDirect('pristine-unseen-tables.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Count of direct answer with exact same 'title' from SyntexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read all the tables\n",
    "all_tables = {}\n",
    "for path in glob.glob(table_path+'*'):\n",
    "    csv_path = path[-7:]\n",
    "    for f in glob.glob(table_path+csv_path+'/*.tsv'):\n",
    "        filename = f.split('/')[-1]\n",
    "        context = 'csv/{0}/{1}csv'.format(csv_path, filename[:-3])\n",
    "        table = read_table(csv_path + '/' + filename)\n",
    "        all_tables[context] = table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_questions = '../data/questions_syntaxnet/'\n",
    "# Change it to return three output: w_word, index of w_word, index of keyword\n",
    "def findTitle(syntaxnet_info):\n",
    "    root, word_info, hierarchy = syntaxnet_info\n",
    "    # find the wh- word\n",
    "    wh_index = -1\n",
    "    for index, word in enumerate(word_info):\n",
    "        if word[2][0] == 'W':\n",
    "            wh_index = index\n",
    "            break\n",
    "    if wh_index == -1:\n",
    "        return None, wh_index, None\n",
    "    # find the 'closest' noun\n",
    "    frontiers, visited = [wh_index], set([wh_index])\n",
    "    while len(frontiers)!=0:\n",
    "        frontier = frontiers.pop()\n",
    "        if word_info[frontier][2][0] == 'N':\n",
    "            return word_info[frontier][0], wh_index, frontier\n",
    "        # children\n",
    "        for i in hierarchy[frontier]:\n",
    "            if i not in visited:\n",
    "                frontiers.insert(0, i)\n",
    "                visited.add(i)\n",
    "        # parent\n",
    "        for i, h in enumerate(hierarchy):\n",
    "            if frontier in h:\n",
    "                if i not in visited:\n",
    "                    frontiers.insert(0, i)\n",
    "                    visited.add(i)\n",
    "                break\n",
    "    return None, wh_index, None\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / np.sqrt(np.dot(v1,v1)*np.dot(v2,v2))\n",
    "\n",
    "def findNearest(word, keys):\n",
    "    if word == None:\n",
    "        return ''\n",
    "    if word.lower() not in word_vectors:\n",
    "        return word\n",
    "    max_sim, result = 0, ''\n",
    "    v1 = word_vectors[word.lower()]\n",
    "    for key in keys:\n",
    "        v2 = np.zeros(v1.shape)\n",
    "        v2 += sum([word_vectors[k.lower()] for k in key.split() if k.lower() in word_vectors])\n",
    "        sim = cosine_similarity(v1, v2)\n",
    "        max_sim, result = (max_sim, result) if sim <= max_sim else (sim, key)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def statsCountExactTitle(filename):\n",
    "    has, hasNot, questions = [], [], None\n",
    "    \n",
    "    with open(data_questions+filename, 'rb') as f:\n",
    "        questions = pickle.load(f)\n",
    "    for id, utterance, context, targetValue, syntax in questions[5:]:\n",
    "        title_question, _, _ = findTitle(syntax)\n",
    "        \n",
    "        title_table = []\n",
    "        for title, elements in all_tables[context].items():\n",
    "            if targetValue in elements:\n",
    "                title_table += [title]\n",
    "        nearest_title = findNearest(title_question, all_tables[context].keys())\n",
    "        if nearest_title in title_table:\n",
    "            has.append(id)\n",
    "        else:\n",
    "            #if len(title_table)!=0:\n",
    "            #    print 'Question is: ' + utterance\n",
    "            #    print '\\tTitle from question is: {0}'.format(title_question)\n",
    "            #    print '\\tNearest title is: {0}'.format(nearest_title)\n",
    "            #    print '\\tTitles from table are:  {0}'.format(title_table)\n",
    "            hasNot.append(id)\n",
    "    print filename\n",
    "    print '  Answer/target of {}% of questions can directly be found in the corresponding table.'.format(\n",
    "        float(len(has))/(len(has)+len(hasNot))*100)\n",
    "    print '  # of questions has directed answer = {0}'.format(len(has))\n",
    "    print '  # of questions has no directed answer = {0}'.format(len(hasNot))\n",
    "    return (has, hasNot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the answer (assume the column is already correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hsinya's algorithm for finding correct row\n",
    "def cos_similarity(w1, w2, word_vectors=word_vectors):\n",
    "    w1, w2 = w1.lower(), w2.lower()\n",
    "    if w1==w2: return 1.0\n",
    "    if w1 not in word_vectors or w2 not in word_vectors:\n",
    "        return 0.0\n",
    "    v1, v2 = word_vectors[w1], word_vectors[w2]\n",
    "    return max(0.0, np.dot(v1,v2))/max(0.01, np.sqrt(np.dot(v1,v1)*np.dot(v2,v2)))\n",
    "\n",
    "def findClue(syntaxnet_info, w_index, title_index):\n",
    "    root, word_info, hierarchy = syntaxnet_info\n",
    "    noun_list = []\n",
    "    # Find all of the nouns in the sentence\n",
    "    for index, word in enumerate(word_info):\n",
    "        if word[2][0] == 'N' and index != title_index:\n",
    "            noun_list.append(word[0])\n",
    "    return noun_list\n",
    "    \n",
    "def findNearestCluePair(clue_list, col_list):\n",
    "    cos_dist, clue_word, col_word = 0, '', ''\n",
    "    for clue in clue_list:\n",
    "        for column in col_list:\n",
    "            dist_list = [0.0]\n",
    "            for column_split in column.split():                   \n",
    "                dist_list.append(cos_similarity(clue, column_split))\n",
    "            if max(dist_list) > cos_dist:\n",
    "                cos_dist, clue_word, col_word = max(dist_list), clue, column\n",
    "    return cos_dist, clue_word, col_word\n",
    "\n",
    "def findAns(clue, ans_column_word, clue_column_word, ans_table):\n",
    "    ans_column = ans_table[ans_column_word]\n",
    "    clue_column = ans_table[clue_column_word]\n",
    "    max_similar, ans_index = 0, 0\n",
    "    for index, clue_cell in enumerate(clue_column):\n",
    "        cos_dist, _, cell_word = findNearestCluePair([clue], clue_cell.split())\n",
    "        if cos_dist > max_similar and index < len(ans_column):\n",
    "            max_similar, ans_index = cos_dist, index\n",
    "    return ans_column[ans_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def statsCountExactCell(filename):\n",
    "    # 3 categories, right answer, wrong answer, wrong column\n",
    "    right, wrong, wrongColumn, questions = [], [], [], None   \n",
    "    with open(data_questions+filename, 'rb') as f:\n",
    "        questions = pickle.load(f)\n",
    "    for id, utterance, context, targetValue, syntax in questions[5:]:\n",
    "        # Find key word for what the question is asking for (column)\n",
    "        title_question, wh_index, title_index= findTitle(syntax)        \n",
    "        title_table = []\n",
    "        for title, elements in all_tables[context].items():\n",
    "            if targetValue in elements:\n",
    "                title_table += [title]        \n",
    "        nearest_title = findNearest(title_question, all_tables[context].keys())\n",
    "        if nearest_title in title_table:\n",
    "            clue_words = findClue(syntax, wh_index, title_index)\n",
    "            table_cols = all_tables[context].keys()\n",
    "            _, clue, clue_column = findNearestCluePair(clue_words, table_cols)\n",
    "            if len(clue_words) == 0 or clue == '' or clue_column == '':\n",
    "                wrong.append(id)\n",
    "            else:\n",
    "                answer = findAns(clue, nearest_title, clue_column, all_tables[context])\n",
    "                if targetValue == answer:\n",
    "                    right.append(id)\n",
    "                else:\n",
    "                    wrong.append(id)\n",
    "        else:\n",
    "            wrongColumn.append(id)\n",
    "    print filename\n",
    "    print '  {}% of questions with correct column have correct answer.'.format(float(len(right))/(len(right)+len(wrong))*100)\n",
    "    print '  {}% of questions have correct column.'.format(float(len(right)+len(wrong))/(len(right)+len(wrong)+len(wrongColumn))*100)  \n",
    "    print '  {}% of questions have correct answer.'.format(float(len(right))/(len(right)+len(wrong)+len(wrongColumn))*100)\n",
    "    print '  Right answer: {}'.format(len(right))\n",
    "    print '  Wrong answer: {}'.format(len(wrong))\n",
    "    print '  Not the correct column: {}'.format(len(wrongColumn))\n",
    "    return (right, wrong, wrongColumn)                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuweilin/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:34: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.pkl\n",
      "  12.5868725869% of questions with correct column have correct answer.\n",
      "  27.4616526472% of questions have correct column.\n",
      "  3.45656322895% of questions have correct answer.\n",
      "  Right answer: 489\n",
      "  Wrong answer: 3396\n",
      "  Not the correct column: 10262\n",
      "seen-tables.pkl\n",
      "  11.9072708114% of questions with correct column have correct answer.\n",
      "  26.8686296716% of questions have correct column.\n",
      "  3.1993204983% of questions have correct answer.\n",
      "  Right answer: 113\n",
      "  Wrong answer: 836\n",
      "  Not the correct column: 2583\n",
      "unseen-tables.pkl\n",
      "  14.9026248942% of questions with correct column have correct answer.\n",
      "  27.2182530537% of questions have correct column.\n",
      "  4.05623415534% of questions have correct answer.\n",
      "  Right answer: 176\n",
      "  Wrong answer: 1005\n",
      "  Not the correct column: 3158\n"
     ]
    }
   ],
   "source": [
    "_ = statsCountExactCell('training.pkl')\n",
    "_ = statsCountExactCell('seen-tables.pkl')\n",
    "_ = statsCountExactCell('unseen-tables.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findAnswerAlan(filename):\n",
    "    has, hasNot, questions = [], [], None\n",
    "    \n",
    "    with open(data_questions+filename, 'rb') as f:\n",
    "        questions = pickle.load(f)\n",
    "    for id, utterance, context, targetValue, syntax in questions[5:]:\n",
    "        title_question = findTitle(syntax)\n",
    "        \n",
    "        title_table = []\n",
    "        for title, elements in all_tables[context].items():\n",
    "            if targetValue in elements:\n",
    "                title_table += [title.lower()]\n",
    "        \n",
    "        nearest_title = findNearest(title_question, all_tables[context].keys())\n",
    "        if nearest_title in title_table:\n",
    "            # 1. syntax: root, word_info, hierarchy\n",
    "            # 2. title_table\n",
    "            # 3. nearest_title: the right column\n",
    "            # 4. original table: all_tables[context] --> need context\n",
    "            # 5. Goal: find the right row\n",
    "            # 6. Goal: output the answer\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'statsCountExactTitle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ed54ccda2dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatsCountExactTitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatsCountExactTitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seen-tables.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatsCountExactTitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unseen-tables.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'statsCountExactTitle' is not defined"
     ]
    }
   ],
   "source": [
    "_ = statsCountExactTitle('training.pkl')\n",
    "_ = statsCountExactTitle('seen-tables.pkl')\n",
    "_ = statsCountExactTitle('unseen-tables.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SyntexNet parsed questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/questions_syntaxnet/seen-tables.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Usage of Word Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1, v2: 0.242447507192 0.122884366123\n",
      "v1, v3: 0.801685509133 0.230863920704\n"
     ]
    }
   ],
   "source": [
    "v1 = word_vectors['cat']\n",
    "v2 = word_vectors['earth']\n",
    "v3 = word_vectors['dog']\n",
    "def cosine(v1, v2):\n",
    "    return np.dot(v1, v2)/np.sqrt(np.dot(v1, v1)*np.dot(v2, v2))\n",
    "def distanceInverse(v1,v2):\n",
    "    return 1.0/np.sqrt(np.sum([x**2 for x in v1-v2]))\n",
    "\n",
    "print 'v1, v2:', cosine(v1, v2), distanceInverse(v1,v2)\n",
    "print 'v1, v3:', cosine(v1, v3), distanceInverse(v1,v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
