{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cPickle as pickle\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read all the tables\n",
    "table_path = '../data/WikiTableQuestions/csv/'\n",
    "\n",
    "def clean_text(s):\n",
    "    return s.lower().replace('\\\\n', ' ').replace('\\n', '')\n",
    "    \n",
    "def read_table(filename, path=table_path, delimiter='\\t'):\n",
    "    ''' given the path and file name of certain table, \n",
    "        return a dictionary with {head-title: [elements]} \n",
    "    '''\n",
    "    result = {}\n",
    "    with open(path+filename, 'r') as f:\n",
    "        heads, head_index = f.readline().split(delimiter), []\n",
    "        for head in heads:\n",
    "            head = clean_text(head)\n",
    "            result[head] = []\n",
    "            head_index.append(head)\n",
    "        \n",
    "        for line in f:\n",
    "            for head, ele in zip(head_index, line.split(delimiter)):\n",
    "                ele = clean_text(ele)\n",
    "                result[head].append(ele)\n",
    "    return result\n",
    "\n",
    "def read_all_tables(table_path=table_path):\n",
    "    all_tables = {}\n",
    "    for path in glob.glob(table_path+'*'):\n",
    "        csv_path = path[-7:]\n",
    "        for f in glob.glob(table_path+csv_path+'/*.tsv'):\n",
    "            filename = f.split('/')[-1]\n",
    "            context = 'csv/{0}/{1}csv'.format(csv_path, filename[:-3])\n",
    "            table = read_table(csv_path + '/' + filename)\n",
    "            all_tables[context] = table\n",
    "    return all_tables\n",
    "\n",
    "all_tables = read_all_tables()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Preprocessing, not need to re-run\n",
    "Proprocess all the questions to format:  \n",
    "{ 'id': {\n",
    "      'question': question (string),  \n",
    "      'table_id': table_id (string),  \n",
    "      'answer': answer (string),  \n",
    "      'answer_position': {column name: [row indices]} # columns, row indices: might have more than one matches,  \n",
    "      'syntax': {  \n",
    "          index: {word, tag, pos, role, parent, [child]}\n",
    "      }  \n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_answer(answer, table):\n",
    "    ''' return answer_position: {column name: [row indices]}\n",
    "    '''\n",
    "    result = {}\n",
    "    for head in table:\n",
    "        indices = [i for i, text in enumerate(table[head]) if text==answer.lower()]\n",
    "        if len(indices)!=0:\n",
    "            result[head] = indices\n",
    "    return result\n",
    "\n",
    "def process_syntax(syntax):\n",
    "    result = {}\n",
    "    for index, info in enumerate(syntax[1]):\n",
    "        result[index] = {'word': info[0], 'tag': info[1], 'pos': info[2], \n",
    "                         'role': info[3], 'parent': None, 'children': []}\n",
    "    for index, children in enumerate(syntax[2]):\n",
    "        for child in children:\n",
    "            result[index]['children'].append(child)\n",
    "            result[child]['parent'] = index\n",
    "    return result\n",
    "    \n",
    "def question_preprocess(loadfile, savefile):\n",
    "    with open(loadfile, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    result = {}\n",
    "    for info in data:\n",
    "        id, question, table_id, answer, syntax = info\n",
    "        answer_position = find_answer(answer, all_tables[table_id])\n",
    "        syntax = process_syntax(syntax)\n",
    "        result[id] = {\n",
    "            'question': question,\n",
    "            'table_id': table_id,\n",
    "            'answer': answer,\n",
    "            'answer_position': answer_position,\n",
    "            'syntax': syntax\n",
    "        }\n",
    "    with open(savefile, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syntaxnet = '../data/questions_syntaxnet/'\n",
    "questions = '../data/PreprocessedQuestions/'\n",
    "files = ['training.pkl', 'seen-tables.pkl', 'unseen-tables.pkl']\n",
    "for filename in files:\n",
    "    question_preprocess(syntaxnet+filename, questions+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Astarto',\n",
       " 'answer_position': {'title': [16]},\n",
       " 'question': 'which opera has the most acts, la fortezza al cimento or astarto?',\n",
       " 'syntax': {0: {'children': [],\n",
       "   'parent': 1,\n",
       "   'pos': 'WDT',\n",
       "   'role': 'det',\n",
       "   'tag': 'DET',\n",
       "   'word': 'which'},\n",
       "  1: {'children': [0],\n",
       "   'parent': 2,\n",
       "   'pos': 'NN',\n",
       "   'role': 'nsubj',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'opera'},\n",
       "  2: {'children': [1, 5, 13],\n",
       "   'parent': None,\n",
       "   'pos': 'VBZ',\n",
       "   'role': 'ROOT',\n",
       "   'tag': 'VERB',\n",
       "   'word': 'has'},\n",
       "  3: {'children': [],\n",
       "   'parent': 5,\n",
       "   'pos': 'DT',\n",
       "   'role': 'det',\n",
       "   'tag': 'DET',\n",
       "   'word': 'the'},\n",
       "  4: {'children': [],\n",
       "   'parent': 5,\n",
       "   'pos': 'JJS',\n",
       "   'role': 'amod',\n",
       "   'tag': 'ADJ',\n",
       "   'word': 'most'},\n",
       "  5: {'children': [3, 4, 6, 10],\n",
       "   'parent': 2,\n",
       "   'pos': 'NNS',\n",
       "   'role': 'dobj',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'acts'},\n",
       "  6: {'children': [],\n",
       "   'parent': 5,\n",
       "   'pos': ',',\n",
       "   'role': 'punct',\n",
       "   'tag': '.',\n",
       "   'word': ','},\n",
       "  7: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'FW',\n",
       "   'role': 'advmod',\n",
       "   'tag': 'X',\n",
       "   'word': 'la'},\n",
       "  8: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'FW',\n",
       "   'role': 'nn',\n",
       "   'tag': 'X',\n",
       "   'word': 'fortezza'},\n",
       "  9: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'NNP',\n",
       "   'role': 'nn',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'al'},\n",
       "  10: {'children': [7, 8, 9, 11, 12],\n",
       "   'parent': 5,\n",
       "   'pos': 'NNP',\n",
       "   'role': 'appos',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'cimento'},\n",
       "  11: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'CC',\n",
       "   'role': 'cc',\n",
       "   'tag': 'CONJ',\n",
       "   'word': 'or'},\n",
       "  12: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'NN',\n",
       "   'role': 'conj',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'astarto'},\n",
       "  13: {'children': [],\n",
       "   'parent': 2,\n",
       "   'pos': '.',\n",
       "   'role': 'punct',\n",
       "   'tag': '.',\n",
       "   'word': '?'}},\n",
       " 'table_id': 'csv/204-csv/104.csv'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to load data + data schema\n",
    "with open('../data/PreprocessedQuestions/training.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "data[data.keys()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Find the correct column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 800000th word\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e0955cfddfae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m'Processing {0}th word'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mword_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load word embedding\n",
    "word_vectors = {}\n",
    "with open('../glove.840B.300d.txt', 'r') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        i += 1\n",
    "        if i % 100000==0: \n",
    "            clear_output()\n",
    "            print 'Processing {0}th word'.format(i)\n",
    "        info = line.split()\n",
    "        word_vectors[info[0]] = np.array([float(x) for x in info[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the 'W*' word, return the index\n",
    "def find_w(syntax):\n",
    "    for index in syntax:\n",
    "        if syntax[index]['pos'][0] == 'W':\n",
    "            return index\n",
    "    return -1 # could not find w word\n",
    "\n",
    "# find the 'nearest' noun of 'W*' word\n",
    "def find_answer_noun(syntax):\n",
    "    w_index = find_w(syntax)\n",
    "    if syntax[w_index]['pos'][0] == 'N':\n",
    "        return w_index\n",
    "    \n",
    "    parent = syntax[w_index]['parent']\n",
    "    children = syntax[w_index]['children']\n",
    "    que = children\n",
    "    que.append(parent)\n",
    "    visited = [w_index, None]\n",
    "    \n",
    "    # Use BFS to find the nearest noun\n",
    "    while len(que) != 0:\n",
    "        index = que.pop(0)\n",
    "        if index == None: continue\n",
    "        if index not in visited:\n",
    "            visited.append(index)\n",
    "            if syntax[index]['pos'][0] == 'N':\n",
    "                return index\n",
    "        new_child = syntax[index]['children']\n",
    "        for child in new_child:\n",
    "            if child not in visited:\n",
    "                que.append(child)\n",
    "        new_parent = syntax[index]['parent']\n",
    "        if new_parent not in visited:\n",
    "            que.append(new_parent)\n",
    "    return -1 # cound not find closest noun\n",
    "\n",
    "# for different 'W*' word, given the object key word\n",
    "def find_object_word(syntax):\n",
    "    w_index = find_w(syntax)\n",
    "    if w_index == -1: return ''\n",
    "    \n",
    "    w_word = syntax[w_index]['word']\n",
    "    if w_word == 'when': return 'year'\n",
    "    if w_word == 'who': return 'name'\n",
    "    if w_word == 'where': return 'location'\n",
    "    \n",
    "    # If w_word == 'how' or 'what'\n",
    "    index = find_answer_noun(syntax)\n",
    "    if index == -1: return ''\n",
    "    \n",
    "    return syntax[index]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(w1, w2, word_vectors=word_vectors):\n",
    "    w1, w2 = w1.lower().split(), w2.lower().split()\n",
    "    dist = 0.0\n",
    "    for ww1 in w1:\n",
    "        if ww1 not in word_vectors: continue\n",
    "        for ww2 in w2:\n",
    "            if ww1 == ww2: return 1.0\n",
    "            if ww2 not in word_vectors: continue\n",
    "            v1, v2 = word_vectors[ww1], word_vectors[ww2]\n",
    "            product = np.dot(v1,v2)\n",
    "            length = np.sqrt(np.dot(v1,v1)*np.dot(v2,v2))\n",
    "            if length == 0: return 1.0\n",
    "            dist = max(dist, product/length)                \n",
    "    return dist\n",
    "\n",
    "def distance_similarity(w1, w2, word_vectors=word_vectors):\n",
    "    w1, w2 = w1.lower(), w2.lower()\n",
    "    if w1==w2: return float('inf')\n",
    "    if w1 not in word_vectors or w2 not in word_vectors:\n",
    "        return 0.0\n",
    "    v1, v2 = word_vectors[w1], word_vectors[w2]\n",
    "    return 1.0/max(0.01, np.sqrt(np.sum((v1-v2)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rank_head_answer_noun_similarity(question, similarity_function = cosine_similarity):\n",
    "    syntax = question['syntax']\n",
    "    answer_noun = find_object_word(syntax)\n",
    "    \n",
    "    if question['answer_position'] == {}:\n",
    "        return (-1, None, answer_noun)\n",
    "    elif answer_noun == '':\n",
    "        return (-2, None, '')\n",
    "    \n",
    "    head = question['answer_position'].keys()[0]\n",
    "    similarity = similarity_function(answer_noun, head)\n",
    "    \n",
    "    result = (-1, None, answer_noun)\n",
    "    for h in all_tables[question['table_id']].keys():\n",
    "        sim = similarity_function(answer_noun, h)\n",
    "        if sim >= similarity: \n",
    "            result = (result[0]+1, h, answer_noun)\n",
    "    if result[1].lower()==head.lower():\n",
    "        result = (0, head, answer_noun)\n",
    "    #w_index = find_w(syntax)\n",
    "    #w_word = syntax[w_index]['word']\n",
    "    #print w_word, result\n",
    "    return result\n",
    "\n",
    "def rank_distribution(questions):\n",
    "    return [(id, rank_head_answer_noun_similarity(question)) for id, question in questions.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJFVjsJAkSdX8f+ec/OkYsGKdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12279b0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Brief Analysis \n",
    "with open('../data/PreprocessedQuestions/training.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "dist = rank_distribution(data)\n",
    "x = [a[1][0] for a in dist]\n",
    "_ = plt.hist(x, bins=max(x)-min(x))\n",
    "\n",
    "# -1: answer cannot be directly found\n",
    "# -2: can't find w word or can't find answer noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We could correctly find 60.7% of clolumns in training.\n",
      "\t6015 4941 3196\n",
      "\n",
      "We could correctly find 60.3% of clolumns in seen-tables.\n",
      "\t1536 1207 794\n",
      "\n",
      "We could correctly find 60.4% of clolumns in unseen-tables.\n",
      "\t1851 1507 986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions_path = '../data/PreprocessedQuestions/'\n",
    "files = ['training.pkl', 'seen-tables.pkl', 'unseen-tables.pkl']\n",
    "column_rank = {}\n",
    "for filename in files:\n",
    "    with open(questions_path+filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    dist = rank_distribution(data)\n",
    "    \n",
    "    column_rank[filename[:-4]] = dist\n",
    "    \n",
    "    x = [a[1][0] for a in dist]\n",
    "    \n",
    "    count_nouns = 0\n",
    "    count_ans = 0\n",
    "    \n",
    "    for number in x:\n",
    "        if number < 0:\n",
    "            count_nouns += 1\n",
    "        else:\n",
    "            count_ans += 1\n",
    "    \n",
    "    print 'We could correctly find {0:.1f}% of clolumns in {1}.'.format(100*float(sum(np.array(x)==0))/count_ans, filename[:-4])\n",
    "    print '\\t', sum(np.array(x)<0), sum(np.array(x)==0), sum(np.array(x)>0)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The questions that get wrong columns\n",
    "def str_question_with_wrong_columns(question, result):\n",
    "    if result[0]==0:\n",
    "        return None\n",
    "    if result[0]<0:\n",
    "        return None\n",
    "    return 'Answer Column: {0}, Predict Column: {1}, Rank: {2}'.format(\n",
    "        question['answer_position'].keys()[0], result[1], result[0]+1)\n",
    "\n",
    "def debug(filename):\n",
    "    with open(questions_path+filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print filename[:-4], ':'\n",
    "\n",
    "    w_list = ['who', 'when', 'what', 'where', 'how', 'why']\n",
    "    w_dict = {'who':0, 'when':0, 'what':0, 'where':0, 'how':0, 'why':0, 'other':0}\n",
    "    w_dict_wrong = {'who':0, 'when':0, 'what':0, 'where':0, 'how':0, 'why':0, 'other':0}\n",
    "    \n",
    "    for id, result in column_rank[filename[:-4]]:\n",
    "        question = data[id]\n",
    "        debug_str = str_question_with_wrong_columns(question, result)\n",
    "        w_word = question['syntax'][0]['word']\n",
    "            \n",
    "        if result[0] == 0:\n",
    "            if w_word in w_list:\n",
    "                w_dict[w_word]+=1\n",
    "            else:\n",
    "                w_dict['other']+=1\n",
    "        if debug_str is not None:\n",
    "            if w_word in w_list:\n",
    "                w_dict_wrong[w_word]+=1\n",
    "            else:\n",
    "                w_dict_wrong['other']+=1\n",
    "            #print ' \\tAnswer Noun: ' + result[2]\n",
    "            #print ' \\t' + debug_str\n",
    "            #print ' \\t' + question['question']\n",
    "            #print\n",
    "    for key in w_dict:\n",
    "        print key, w_dict[key], w_dict_wrong[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training :\n",
      "what 1663 837\n",
      "who 577 449\n",
      "when 85 17\n",
      "how 476 1000\n",
      "other 2093 877\n",
      "where 47 15\n",
      "why 0 1\n"
     ]
    }
   ],
   "source": [
    "filename = 'training.pkl'\n",
    "debug(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen-tables :\n",
      "what 416 200\n",
      "who 136 121\n",
      "when 27 3\n",
      "how 113 237\n",
      "other 505 227\n",
      "where 10 6\n",
      "why 0 0\n"
     ]
    }
   ],
   "source": [
    "filename = 'seen-tables.pkl'\n",
    "debug(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen-tables :\n",
      "what 489 238\n",
      "who 188 186\n",
      "when 32 5\n",
      "how 187 303\n",
      "other 589 252\n",
      "where 22 2\n",
      "why 0 0\n"
     ]
    }
   ],
   "source": [
    "filename = 'unseen-tables.pkl'\n",
    "debug(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Find the answer (clue column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cos_sim_forComplex(w1_list, w2):\n",
    "    w2_complex = w2.split()\n",
    "    dist = []\n",
    "    temp_dist = 0.0\n",
    "    for word2 in w2_complex:        \n",
    "        for word1 in w1_list:\n",
    "            if word2 not in word_vectors: break\n",
    "            if word1 not in word_vectors: continue\n",
    "            if word2 == word1: \n",
    "                temp_dist = 1.0\n",
    "                break\n",
    "            v1, v2 = word_vectors[word1], word_vectors[word2]\n",
    "            product = np.dot(v1,v2)\n",
    "            length = np.sqrt(np.dot(v1,v1)*np.dot(v2,v2))\n",
    "            if length == 0: \n",
    "                temp_dist = 1.0\n",
    "                break\n",
    "            temp_dist = max(temp_dist, product/length)\n",
    "        dist.append(temp_dist)\n",
    "        temp_dist = 0.0\n",
    "    return sum(dist)\n",
    "    \n",
    "\n",
    "def findClueList(syntax, key_word):\n",
    "    noun_list = []\n",
    "    # Find all of the nouns in the sentence\n",
    "    for key in syntax:\n",
    "        if syntax[key]['pos'][0] == 'N' and syntax[key]['word'] != key_word:\n",
    "            noun_list.append(syntax[key]['word'])\n",
    "    return noun_list\n",
    "    \n",
    "def findNearestCluePair(clue_list, col_list, similarity_function = cosine_similarity):\n",
    "    cos_dist, column_word = 0.0, ''\n",
    "    \n",
    "    for column in col_list:\n",
    "        dist = cos_sim_forComplex(clue_list, column)\n",
    "        if dist > cos_dist:\n",
    "            cos_dist, column_word = dist, column    \n",
    "    return column_word\n",
    "\n",
    "def findAns(question, column, key_word):    \n",
    "    ans_table = all_tables[question['table_id']]\n",
    "    ans_column = ans_table[column]\n",
    "    syntax = question['syntax']\n",
    "    \n",
    "    question_set = set(question['question'].split())\n",
    "    \n",
    "    clue_list = findClueList(syntax, key_word)\n",
    "    column_list = ans_table.keys()\n",
    "    column_list.remove(column)\n",
    "    \n",
    "    clue_column = findNearestCluePair(clue_list, column_list)\n",
    "    \n",
    "    if clue_column == '': return '', question['answer']\n",
    "    clue_column_list = ans_table[clue_column]\n",
    "    ans_index = 0\n",
    "    for index, clue_cell in enumerate(clue_column_list):\n",
    "        clue_cell_set = set(clue_cell.split())\n",
    "        if clue_cell_set.issubset(question_set) and index < len(ans_column):\n",
    "            ans_index = index\n",
    "            break\n",
    "    return ans_column[ans_index], question['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printResult(filename, n_a, wrong_column, wrong_answer, right_answer):\n",
    "    print filename\n",
    "    print '  {}% of questions with correct column have correct answer.'.format(float(right_answer)/(wrong_answer+right_answer)*100)\n",
    "    print '  {}% of questions have correct column.'.format(float(wrong_answer+right_answer)/(wrong_answer+right_answer+wrong_column)*100) \n",
    "    print '  {}% of questions have correct answer.'.format(float(right_answer)/(wrong_answer+right_answer+wrong_column)*100)\n",
    "    print '  Right answer: {}'.format(right_answer)\n",
    "    print '  Wrong answer: {}'.format(wrong_answer)\n",
    "    print '  Not the correct column: {}'.format(wrong_column)\n",
    "    print '  Not applicable: {}'.format(n_a)\n",
    "    \n",
    "def answerQuestions(questions):\n",
    "    n_a, wrong_column, wrong_answer, right_answer = 0, 0, 0, 0\n",
    "    for id, question in questions.items():\n",
    "        rank, column, key_word = rank_head_answer_noun_similarity(question)\n",
    "        if rank < 0:\n",
    "            n_a += 1\n",
    "        elif rank > 0:\n",
    "            wrong_column += 1\n",
    "        else: # rank == 0\n",
    "            found, real = findAns(question, column, key_word)\n",
    "            if found.lower() == real.lower():\n",
    "                right_answer += 1\n",
    "            else:\n",
    "                wrong_answer += 1\n",
    "    return (n_a, wrong_column, wrong_answer, right_answer)\n",
    "\n",
    "def processFiles(questions_path, files):    \n",
    "    for filename in files:        \n",
    "        with open(questions_path+filename, 'rb') as f:\n",
    "            questions = pickle.load(f)\n",
    "        n_a, wrong_column, wrong_answer, right_answer = answerQuestions(questions)\n",
    "        printResult(filename, n_a, wrong_column, wrong_answer, right_answer)\n",
    "    return 'Analysis Complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.pkl\n",
      "  21.392430682% of questions with correct column have correct answer.\n",
      "  60.7226250461% of questions have correct column.\n",
      "  12.9900454713% of questions have correct answer.\n",
      "  Right answer: 1057\n",
      "  Wrong answer: 3884\n",
      "  Not the correct column: 3196\n",
      "  Not applicable: 6015\n",
      "seen-tables.pkl\n",
      "  19.9668599834% of questions with correct column have correct answer.\n",
      "  60.31984008% of questions have correct column.\n",
      "  12.043978011% of questions have correct answer.\n",
      "  Right answer: 241\n",
      "  Wrong answer: 966\n",
      "  Not the correct column: 794\n",
      "  Not applicable: 1536\n",
      "unseen-tables.pkl\n",
      "  21.0351692104% of questions with correct column have correct answer.\n",
      "  60.4492579222% of questions have correct column.\n",
      "  12.7156036903% of questions have correct answer.\n",
      "  Right answer: 317\n",
      "  Wrong answer: 1190\n",
      "  Not the correct column: 986\n",
      "  Not applicable: 1851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Analysis Complete'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_path = '../data/PreprocessedQuestions/'\n",
    "files = ['training.pkl', 'seen-tables.pkl', 'unseen-tables.pkl']\n",
    "processFiles(questions_path, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
