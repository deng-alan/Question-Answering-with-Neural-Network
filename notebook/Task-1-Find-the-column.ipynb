{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cPickle as pickle\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read all the tables\n",
    "table_path = '../data/WikiTableQuestions/csv/'\n",
    "\n",
    "def clean_text(s):\n",
    "    return s.lower().replace('\\\\n', ' ').replace('\\n', '')\n",
    "    \n",
    "def read_table(filename, path=table_path, delimiter='\\t'):\n",
    "    ''' given the path and file name of certain table, \n",
    "        return a dictionary with {head-title: [elements]} \n",
    "    '''\n",
    "    result = {}\n",
    "    with open(path+filename, 'r') as f:\n",
    "        heads, head_index = f.readline().split(delimiter), []\n",
    "        for head in heads:\n",
    "            head = clean_text(head)\n",
    "            result[head] = []\n",
    "            head_index.append(head)\n",
    "        \n",
    "        for line in f:\n",
    "            for head, ele in zip(head_index, line.split(delimiter)):\n",
    "                ele = clean_text(ele)\n",
    "                result[head].append(ele)\n",
    "    return result\n",
    "\n",
    "def read_all_tables(table_path=table_path):\n",
    "    all_tables = {}\n",
    "    for path in glob.glob(table_path+'*'):\n",
    "        csv_path = path[-7:]\n",
    "        for f in glob.glob(table_path+csv_path+'/*.tsv'):\n",
    "            filename = f.split('/')[-1]\n",
    "            context = 'csv/{0}/{1}csv'.format(csv_path, filename[:-3])\n",
    "            table = read_table(csv_path + '/' + filename)\n",
    "            all_tables[context] = table\n",
    "    return all_tables\n",
    "\n",
    "all_tables = read_all_tables()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Preprocessing, not need to re-run\n",
    "Proprocess all the questions to format:  \n",
    "{ 'id': {\n",
    "      'question': question (string),  \n",
    "      'table_id': table_id (string),  \n",
    "      'answer': answer (string),  \n",
    "      'answer_position': {column name: [row indices]} # columns, row indices: might have more than one matches,  \n",
    "      'syntax': {  \n",
    "          index: {word, tag, pos, role, parent, [child]}\n",
    "      }  \n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_answer(answer, table):\n",
    "    ''' return answer_position: {column name: [row indices]}\n",
    "    '''\n",
    "    result = {}\n",
    "    for head in table:\n",
    "        indices = [i for i, text in enumerate(table[head]) if text==answer.lower()]\n",
    "        if len(indices)!=0:\n",
    "            result[head] = indices\n",
    "    return result\n",
    "\n",
    "def process_syntax(syntax):\n",
    "    result = {}\n",
    "    for index, info in enumerate(syntax[1]):\n",
    "        result[index] = {'word': info[0], 'tag': info[1], 'pos': info[2], \n",
    "                         'role': info[3], 'parent': None, 'children': []}\n",
    "    for index, children in enumerate(syntax[2]):\n",
    "        for child in children:\n",
    "            result[index]['children'].append(child)\n",
    "            result[child]['parent'] = index\n",
    "    return result\n",
    "    \n",
    "def question_preprocess(loadfile, savefile):\n",
    "    with open(loadfile, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    result = {}\n",
    "    for info in data:\n",
    "        id, question, table_id, answer, syntax = info\n",
    "        answer_position = find_answer(answer, all_tables[table_id])\n",
    "        syntax = process_syntax(syntax)\n",
    "        result[id] = {\n",
    "            'question': question,\n",
    "            'table_id': table_id,\n",
    "            'answer': answer,\n",
    "            'answer_position': answer_position,\n",
    "            'syntax': syntax\n",
    "        }\n",
    "    with open(savefile, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syntaxnet = '../data/questions_syntaxnet/'\n",
    "questions = '../data/PreprocessedQuestions/'\n",
    "files = ['training.pkl', 'seen-tables.pkl', 'unseen-tables.pkl']\n",
    "for filename in files:\n",
    "    question_preprocess(syntaxnet+filename, questions+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Astarto',\n",
       " 'answer_position': {'title': [16]},\n",
       " 'question': 'which opera has the most acts, la fortezza al cimento or astarto?',\n",
       " 'syntax': {0: {'children': [],\n",
       "   'parent': 1,\n",
       "   'pos': 'WDT',\n",
       "   'role': 'det',\n",
       "   'tag': 'DET',\n",
       "   'word': 'which'},\n",
       "  1: {'children': [0],\n",
       "   'parent': 2,\n",
       "   'pos': 'NN',\n",
       "   'role': 'nsubj',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'opera'},\n",
       "  2: {'children': [1, 5, 13],\n",
       "   'parent': None,\n",
       "   'pos': 'VBZ',\n",
       "   'role': 'ROOT',\n",
       "   'tag': 'VERB',\n",
       "   'word': 'has'},\n",
       "  3: {'children': [],\n",
       "   'parent': 5,\n",
       "   'pos': 'DT',\n",
       "   'role': 'det',\n",
       "   'tag': 'DET',\n",
       "   'word': 'the'},\n",
       "  4: {'children': [],\n",
       "   'parent': 5,\n",
       "   'pos': 'JJS',\n",
       "   'role': 'amod',\n",
       "   'tag': 'ADJ',\n",
       "   'word': 'most'},\n",
       "  5: {'children': [3, 4, 6, 10],\n",
       "   'parent': 2,\n",
       "   'pos': 'NNS',\n",
       "   'role': 'dobj',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'acts'},\n",
       "  6: {'children': [],\n",
       "   'parent': 5,\n",
       "   'pos': ',',\n",
       "   'role': 'punct',\n",
       "   'tag': '.',\n",
       "   'word': ','},\n",
       "  7: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'FW',\n",
       "   'role': 'advmod',\n",
       "   'tag': 'X',\n",
       "   'word': 'la'},\n",
       "  8: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'FW',\n",
       "   'role': 'nn',\n",
       "   'tag': 'X',\n",
       "   'word': 'fortezza'},\n",
       "  9: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'NNP',\n",
       "   'role': 'nn',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'al'},\n",
       "  10: {'children': [7, 8, 9, 11, 12],\n",
       "   'parent': 5,\n",
       "   'pos': 'NNP',\n",
       "   'role': 'appos',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'cimento'},\n",
       "  11: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'CC',\n",
       "   'role': 'cc',\n",
       "   'tag': 'CONJ',\n",
       "   'word': 'or'},\n",
       "  12: {'children': [],\n",
       "   'parent': 10,\n",
       "   'pos': 'NN',\n",
       "   'role': 'conj',\n",
       "   'tag': 'NOUN',\n",
       "   'word': 'astarto'},\n",
       "  13: {'children': [],\n",
       "   'parent': 2,\n",
       "   'pos': '.',\n",
       "   'role': 'punct',\n",
       "   'tag': '.',\n",
       "   'word': '?'}},\n",
       " 'table_id': 'csv/204-csv/104.csv'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to load data + data schema\n",
    "with open('../data/PreprocessedQuestions/training.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "data[data.keys()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Find the correct column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2100000th word\n"
     ]
    }
   ],
   "source": [
    "# load word embedding\n",
    "word_vectors = {}\n",
    "with open('../glove.840B.300d.txt', 'r') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        i += 1\n",
    "        if i % 100000==0: \n",
    "            clear_output()\n",
    "            print 'Processing {0}th word'.format(i)\n",
    "        info = line.split()\n",
    "        word_vectors[info[0]] = np.array([float(x) for x in info[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the 'W*' word, return the index\n",
    "def find_w(syntax):\n",
    "    for index in syntax:\n",
    "        if syntax[index]['pos'][0] == 'W':\n",
    "            return index\n",
    "    return -1 # could not find w word\n",
    "\n",
    "# find the 'nearest' noun of 'W*' word\n",
    "def find_answer_noun(syntax):\n",
    "    w_index = find_w(syntax)\n",
    "    if syntax[w_index]['pos'][0] == 'N':\n",
    "        return w_index\n",
    "    \n",
    "    parent = syntax[w_index]['parent']\n",
    "    children = syntax[w_index]['children']\n",
    "    que = children\n",
    "    if parent != None:\n",
    "        que.append(parent)\n",
    "    visited = [w_index]\n",
    "    while len(que) != 0:\n",
    "        index = que.pop(0)\n",
    "        if index == None: continue\n",
    "        if index not in visited:\n",
    "            visited.append(index)\n",
    "            if syntax[index]['pos'][0] == 'N':\n",
    "                return index\n",
    "        new_child = syntax[index]['children']\n",
    "        for child in new_child:\n",
    "            if child not in visited:\n",
    "                que.append(child)\n",
    "        new_parent = syntax[index]['parent']\n",
    "        if new_parent not in visited:\n",
    "            que.append(new_parent)\n",
    "    return -1 # cound not find closest noun\n",
    "\n",
    "# for different 'W*' word, given the object key word\n",
    "def find_object_word(syntax):\n",
    "    w_index = find_w(syntax)\n",
    "    if w_index == -1: return ''\n",
    "    \n",
    "    w_word = syntax[w_index]['word']\n",
    "    if w_word == 'when': return 'year'\n",
    "    if w_word == 'who': return 'name'\n",
    "    if w_word == 'where': return 'location'\n",
    "    #if w_word == 'how':\n",
    "    #    words = ['many', 'much']\n",
    "    #    if syntax[w_index+1]['word'] in words: return 'number'\n",
    "    \n",
    "    index = find_answer_noun(syntax)\n",
    "    if index == -1: return ''\n",
    "    \n",
    "    return syntax[index]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(w1, w2, word_vectors=word_vectors):\n",
    "    w1, w2 = w1.lower().split(), w2.lower().split()\n",
    "    dist = 0.0\n",
    "    for ww1 in w1:\n",
    "        if ww1 not in word_vectors: continue\n",
    "        for ww2 in w2:\n",
    "            if ww1 == ww2: return 1.0\n",
    "            if ww2 not in word_vectors: continue\n",
    "            v1, v2 = word_vectors[ww1], word_vectors[ww2]\n",
    "            product = np.dot(v1,v2)\n",
    "            length = np.sqrt(np.dot(v1,v1)*np.dot(v2,v2))\n",
    "            if length == 0: return 1.0\n",
    "            dist = max(dist, product/length)                \n",
    "    return dist\n",
    "\n",
    "def distance_similarity(w1, w2, word_vectors=word_vectors):\n",
    "    w1, w2 = w1.lower(), w2.lower()\n",
    "    if w1==w2: return float('inf')\n",
    "    if w1 not in word_vectors or w2 not in word_vectors:\n",
    "        return 0.0\n",
    "    v1, v2 = word_vectors[w1], word_vectors[w2]\n",
    "    return 1.0/max(0.01, np.sqrt(np.sum((v1-v2)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rank_head_answer_noun_similarity(question, similarity_function = cosine_similarity):\n",
    "    syntax = question['syntax']\n",
    "    answer_noun = find_object_word(syntax)\n",
    "    \n",
    "    if question['answer_position'] == {}:\n",
    "        return (-1, None, answer_noun)\n",
    "    elif answer_noun == '':\n",
    "        return (-2, None, '')\n",
    "    \n",
    "    head = question['answer_position'].keys()[0]\n",
    "    similarity = similarity_function(answer_noun, head)\n",
    "    \n",
    "    result = (-1, None, answer_noun)\n",
    "    for h in all_tables[question['table_id']].keys():\n",
    "        sim = similarity_function(answer_noun, h)\n",
    "        if sim >= similarity: \n",
    "            result = (result[0]+1, h, answer_noun)\n",
    "    if result[1].lower()==head.lower():\n",
    "        result = (0, head, answer_noun)\n",
    "    #w_index = find_w(syntax)\n",
    "    #w_word = syntax[w_index]['word']\n",
    "    #print w_word, result\n",
    "    return result\n",
    "\n",
    "def rank_distribution(questions):\n",
    "    return [(id, rank_head_answer_noun_similarity(question)) for id, question in questions.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFpIkqRqDhSRJqsZgIUmSqjFYSJKkav4/8r4EYYrOQisAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12269b090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Brief Analysis \n",
    "with open('../data/PreprocessedQuestions/training.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "dist = rank_distribution(data)\n",
    "x = [a[1][0] for a in dist]\n",
    "_ = plt.hist(x, bins=max(x)-min(x))\n",
    "\n",
    "# -1: answer cannot be directly found\n",
    "# -2: can't find w word or can't find answer noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We could correctly find 60.8% of clolumns in training.\n",
      "\t6015 4948 3189\n",
      "\n",
      "We could correctly find 60.5% of clolumns in seen-tables.\n",
      "\t1536 1210 791\n",
      "\n",
      "We could correctly find 60.5% of clolumns in unseen-tables.\n",
      "\t1851 1508 985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions_path = '../data/PreprocessedQuestions/'\n",
    "files = ['training.pkl', 'seen-tables.pkl', 'unseen-tables.pkl']\n",
    "column_rank = {}\n",
    "for filename in files:\n",
    "    with open(questions_path+filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    dist = rank_distribution(data)\n",
    "    \n",
    "    column_rank[filename[:-4]] = dist\n",
    "    \n",
    "    x = [a[1][0] for a in dist]\n",
    "    \n",
    "    count_nouns = 0\n",
    "    count_ans = 0\n",
    "    \n",
    "    for number in x:\n",
    "        if number < 0:\n",
    "            count_nouns += 1\n",
    "        else:\n",
    "            count_ans += 1\n",
    "    \n",
    "    print 'We could correctly find {0:.1f}% of clolumns in {1}.'.format(100*float(sum(np.array(x)==0))/count_ans, filename[:-4])\n",
    "    print '\\t', sum(np.array(x)<0), sum(np.array(x)==0), sum(np.array(x)>0)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The questions that get wrong columns\n",
    "def str_question_with_wrong_columns(question, result):\n",
    "    if result[0]==0:\n",
    "        return None\n",
    "    if result[0]<0:\n",
    "        return None\n",
    "    return 'Answer Column: {0}, Predict Column: {1}, Rank: {2}'.format(\n",
    "        question['answer_position'].keys()[0], result[1], result[0]+1)\n",
    "\n",
    "def debug(filename):\n",
    "    with open(questions_path+filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print filename[:-4], ':'\n",
    "\n",
    "    w_list = ['who', 'when', 'what', 'where', 'how', 'why']\n",
    "    w_dict = {'who':0, 'when':0, 'what':0, 'where':0, 'how':0, 'why':0, 'other':0}\n",
    "    w_dict_wrong = {'who':0, 'when':0, 'what':0, 'where':0, 'how':0, 'why':0, 'other':0}\n",
    "    \n",
    "    for id, result in column_rank[filename[:-4]]:\n",
    "        question = data[id]\n",
    "        debug_str = str_question_with_wrong_columns(question, result)\n",
    "        w_word = question['syntax'][0]['word']\n",
    "            \n",
    "        if result[0] == 0:\n",
    "            if w_word in w_list:\n",
    "                w_dict[w_word]+=1\n",
    "            else:\n",
    "                w_dict['other']+=1\n",
    "        if debug_str is not None:\n",
    "            if w_word in w_list:\n",
    "                w_dict_wrong[w_word]+=1\n",
    "            else:\n",
    "                w_dict_wrong['other']+=1\n",
    "            #print ' Question id: ' + id\n",
    "            #print ' \\tAnswer Noun: ' + result[2]\n",
    "            #print ' \\t' + debug_str\n",
    "            #print ' \\t' + question['question']\n",
    "            #print\n",
    "    for key in w_dict:\n",
    "        print key, w_dict[key], w_dict_wrong[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training :\n",
      "what 1666 834\n",
      "who 577 449\n",
      "when 85 17\n",
      "how 480 996\n",
      "other 2093 877\n",
      "where 47 15\n",
      "why 0 1\n"
     ]
    }
   ],
   "source": [
    "filename = 'training.pkl'\n",
    "debug(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen-tables :\n",
      "what 417 199\n",
      "who 136 121\n",
      "when 27 3\n",
      "how 114 236\n",
      "other 506 226\n",
      "where 10 6\n",
      "why 0 0\n"
     ]
    }
   ],
   "source": [
    "filename = 'seen-tables.pkl'\n",
    "debug(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen-tables :\n",
      "what 489 238\n",
      "who 188 186\n",
      "when 32 5\n",
      "how 187 303\n",
      "other 590 251\n",
      "where 22 2\n",
      "why 0 0\n"
     ]
    }
   ],
   "source": [
    "filename = 'unseen-tables.pkl'\n",
    "debug(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Find the answer (clue column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findClueList(syntax, key_word):\n",
    "    noun_list = []\n",
    "    # Find all of the nouns in the sentence\n",
    "    for key in syntax:\n",
    "        if syntax[key]['pos'][0] == 'N' and syntax[key]['word'] != key_word:\n",
    "            noun_list.append(syntax[key]['word'])\n",
    "        #if syntax[key]['word'] != key_word:\n",
    "        #    noun_list.append(syntax[key]['word'])\n",
    "    return noun_list\n",
    "    \n",
    "def findNearestCluePair(clue_list, col_list, similarity_function = cosine_similarity):\n",
    "    cos_dist, clue_word, column_word = 0.0, '', ''\n",
    "    for clue in clue_list:\n",
    "        for column in col_list:\n",
    "            dist = similarity_function(clue, column)\n",
    "            if dist > cos_dist:\n",
    "                cos_dist, clue_word, column_word = dist, clue, column\n",
    "    return clue_word, column_word\n",
    "\n",
    "def findAns(question, column, key_word):    \n",
    "    ans_table = all_tables[question['table_id']]\n",
    "    ans_column = ans_table[column]\n",
    "    syntax = question['syntax']\n",
    "    \n",
    "    clue_list = findClueList(syntax, key_word)\n",
    "    column_list = ans_table.keys()\n",
    "    column_list.remove(column)\n",
    "    clue_word, clue_column = findNearestCluePair(clue_list, column_list)\n",
    "    \n",
    "    if clue_column == '': return '', question['answer']\n",
    "    clue_column_list = ans_table[clue_column]\n",
    "    max_similar, ans_index = 0.0, 0\n",
    "    for index, clue_cell in enumerate(clue_column_list):\n",
    "        cos_dist = cosine_similarity(clue_word, clue_cell)\n",
    "        if cos_dist >= max_similar and index < len(ans_column):\n",
    "            max_similar, ans_index = cos_dist, index \n",
    "    return ans_column[ans_index], question['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printResult(filename, n_a, wrong_column, wrong_answer, right_answer):\n",
    "    print filename\n",
    "    print '  {}% of questions with correct column have correct answer.'.format(float(right_answer)/(wrong_answer+right_answer)*100)\n",
    "    print '  {}% of questions have correct column.'.format(float(wrong_answer+right_answer)/(wrong_answer+right_answer+wrong_column)*100) \n",
    "    print '  {}% of questions have correct answer.'.format(float(right_answer)/(wrong_answer+right_answer+wrong_column)*100)\n",
    "    print '  Right answer: {}'.format(right_answer)\n",
    "    print '  Wrong answer: {}'.format(wrong_answer)\n",
    "    print '  Not the correct column: {}'.format(wrong_column)\n",
    "    print '  Not applicable: {}'.format(n_a)\n",
    "    \n",
    "def answerQuestions(questions):\n",
    "    n_a, wrong_column, wrong_answer, right_answer = 0, 0, 0, 0\n",
    "    for id, question in questions.items():\n",
    "        rank, column, key_word = rank_head_answer_noun_similarity(question)\n",
    "        if rank < 0:\n",
    "            n_a += 1\n",
    "        elif rank > 0:\n",
    "            wrong_column += 1\n",
    "        else: # rank == 0\n",
    "            found, real = findAns(question, column, key_word)\n",
    "            if found.lower() == real.lower():\n",
    "                right_answer += 1\n",
    "            else:\n",
    "                wrong_answer += 1\n",
    "    return (n_a, wrong_column, wrong_answer, right_answer)\n",
    "\n",
    "def processFiles(questions_path, files):    \n",
    "    for filename in files:        \n",
    "        with open(questions_path+filename, 'rb') as f:\n",
    "            questions = pickle.load(f)\n",
    "        n_a, wrong_column, wrong_answer, right_answer = answerQuestions(questions)\n",
    "        printResult(filename, n_a, wrong_column, wrong_answer, right_answer)\n",
    "    return 0                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.pkl\n",
      "  13.2174616006% of questions with correct column have correct answer.\n",
      "  60.8086518373% of questions have correct column.\n",
      "  8.03736020646% of questions have correct answer.\n",
      "  Right answer: 654\n",
      "  Wrong answer: 4294\n",
      "  Not the correct column: 3189\n",
      "  Not applicable: 6015\n",
      "seen-tables.pkl\n",
      "  14.3801652893% of questions with correct column have correct answer.\n",
      "  60.4697651174% of questions have correct column.\n",
      "  8.69565217391% of questions have correct answer.\n",
      "  Right answer: 174\n",
      "  Wrong answer: 1036\n",
      "  Not the correct column: 791\n",
      "  Not applicable: 1536\n",
      "unseen-tables.pkl\n",
      "  15.3846153846% of questions with correct column have correct answer.\n",
      "  60.4893702367% of questions have correct column.\n",
      "  9.30605695949% of questions have correct answer.\n",
      "  Right answer: 232\n",
      "  Wrong answer: 1276\n",
      "  Not the correct column: 985\n",
      "  Not applicable: 1851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_path = '../data/PreprocessedQuestions/'\n",
    "files = ['training.pkl', 'seen-tables.pkl', 'unseen-tables.pkl']\n",
    "processFiles(questions_path, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
